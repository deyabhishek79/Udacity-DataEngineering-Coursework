{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, from_unixtime, unix_timestamp, to_date\n",
    "import zipfile\n",
    "from pyspark.sql.types import TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_to_folder(log_path, song_path):\n",
    "    with zipfile.ZipFile('data/log-data.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(log_path)\n",
    "    with zipfile.ZipFile('data/song-data.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(song_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = config.get('default', 'AWS_ACCESS_KEY_ID')\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']= config.get('default', 'AWS_SECRET_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://d656cc3e000b:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f28353e21d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = \"s3a://udacity-dend/\"\n",
    "song_data = input_data + 'song_data/*/*/*/*.json'\n",
    "log_path = input_data + 'log_data/*.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = 'data/song_data/song_data/A/A/A/*.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data = spark.read.json(songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "song_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = 'data/log_data/*.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data = spark.read.json(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    # get filepath to song data file\n",
    "    song_data = 'data/song_data/song_data/A/A/A/*.json'\n",
    "    \n",
    "    # read song data file\n",
    "    df = spark.read.json(logs)\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    df.select('song_id', 'title', 'artist_id', 'year', 'duration')\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_table = songs_table.write.partitionBy(['year', 'artist_id']).parquet('data/songs_table.parquet')\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    artists_table = df.select('artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude')\n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    artists_table.write.parquet('data/artist_table.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_filter = log_data.filter(log_data['page'] == 'NextSong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_timestamp = udf(lambda x: str(int(int(x)/1000)))\n",
    "log_filter = log_filter.withColumn('timestamp', get_timestamp(log_filter.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+-------------+------+-------------+--------------------+------+----------+\n",
      "|  artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page|     registration|sessionId|         song|status|           ts|           userAgent|userId| timestamp|\n",
      "+--------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+-------------+------+-------------+--------------------+------+----------+\n",
      "|Harmonia|Logged In|     Ryan|     M|            0|   Smith|655.77751| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|      583|Sehr kosmisch|   200|1542241826796|\"Mozilla/5.0 (X11...|    26|1542241826|\n",
      "+--------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+-------------+------+-------------+--------------------+------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_filter.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df = spark.read.parquet('data/songs_table.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['song_id', 'title', 'duration', 'year', 'artist_id']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artist',\n",
       " 'auth',\n",
       " 'firstName',\n",
       " 'gender',\n",
       " 'itemInSession',\n",
       " 'lastName',\n",
       " 'length',\n",
       " 'level',\n",
       " 'location',\n",
       " 'method',\n",
       " 'page',\n",
       " 'registration',\n",
       " 'sessionId',\n",
       " 'song',\n",
       " 'status',\n",
       " 'ts',\n",
       " 'userAgent',\n",
       " 'userId',\n",
       " 'timestamp']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_filter.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_data(spark, input_data, output_data):\n",
    "    # get filepath to log data file\n",
    "    log_data = 'data/log_data/*.json'\n",
    "\n",
    "    # read log data file\n",
    "    df = spark.read.json(input_data)\n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    df = log_data = spark.read.json(logs)\n",
    "\n",
    "    # extract columns for users table    \n",
    "    users = df.select('userId', 'firstName', 'lastName', 'gender', 'level')\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    users.write.parquet('data/users_table.parquet')\n",
    "\n",
    "    #create timestamp column from original timestamp column\n",
    "    get_timestamp = udf(lambda x: str(int(int(x)/1000)))\n",
    "    df = df.withColumn('timestamp', get_timestamp(log_filter.ts))\n",
    "    get_datetime = udf(lambda x: datetime.fromtimestamp(int(int(x)/1000)))\n",
    "    get_week = udf(lambda x: calendar.day_name[x.weekday()])\n",
    "    get_weekday = udf(lambda x: x.isocalendar()[1])\n",
    "    get_hour = udf(lambda x: x.hour)\n",
    "    get_day = udf(lambda x : x.day)\n",
    "    get_year = udf(lambda x: x.year)\n",
    "    get_month = udf(lambda x: x.month)\n",
    "    \n",
    "    df = df.withColumn('timestamp', get_timestamp(log_filter.ts))\n",
    "    df = df.withColumn('datetime', get_datetime(df.ts))\n",
    "    df = df.withColumn('start_time', get_datetime(df.ts))\n",
    "    df = df.withColumn('hour', get_hour(df.start_time))\n",
    "    df = df.withColumn('day', get_day(df.start_time))\n",
    "    df = df.withColumn('week', get_week(df.start_time))\n",
    "    df = df.withColumn('month', get_month(df.start_time))\n",
    "    df = df.withColumn('year', get_year(df.start_time))\n",
    "    df = df.withColumn('weekday', get_weekday(df.start_time))\n",
    "    \n",
    "    # extract columns to create time table\n",
    "    time_table  = df['start_time', 'hour', 'day', 'week', 'month', 'year', 'weekday']\n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table = time_table.write.partitionBy(['year', 'month']).parquet('data/time_table.parquet')\n",
    "\n",
    "    # read in song data to use for songplays table\n",
    "    song_df = spark.read.parquet('data/songs_table.parquet')\n",
    "    \n",
    "    #join the tables together to have all info needed for the songplays table\n",
    "    df = df.join(song_df, song_df.title == df.song)\n",
    "    \n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "    songplays_table = df.select('ts', 'userId', 'level', 'song_id', 'artist_id', 'sessionId', 'location', 'userAgent')\n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_table.write.parquet('data/songplays_table.parquet')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
